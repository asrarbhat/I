module 1:

    it was just an introduction.

module 2:

    what is deep learning?
        ML based on ANN
    what is Machine Learning?
        it is turning data into numbers and finding patterns
        in those numbers 
        How?
            code and math.
    
    AI contains ML and ML contains DL.

    traditional Programming: input + rules  => output 
    Machine learning       : input + output => rules/approximate rules.

    why ML or DL ?
        for a complex problem,it is hard to think of all rules,Hence let the machine figure out all rules.
    
    you can pretty much do anything with ML.

    Don't use ML if you can build good rule based system. rule 1 in googles machines learning guide.
    
    when rules are hard to figure out,use DL.
    or when a long list of rules.
    if data changes continuously use DL.
    large data to discover insights,use DL.

    first important step is to represent Inputs and Outputs in numbers in a meaninful and
    efficient way.

    google, Google machine learning handbook to see do's and dont's of ML.

    when not to use DL?
        when you need explainability.
        patterns learned are not interpretable.
        when traditional approach is better.
        when errors are unacceptable.
        when you don't have much data.
    
    common ML => for structured data => shallow algorithms
        Random Forest
        Naive Bayes
        Nearest neighbour
        SVM
        etc
    DL => good for unstructured data => text,voice,images etc
        Neural networks
        fully connected Neural networks
        CNN 
        RNN 
        transformers 
        etc
    
    Neural network:
        a network or circuit of neurons.
    
    input -> numerical encoding -> learns representation,patterns,features,weights -> representation of output -> convert to human understandable output.

    Anatomy of NN:
        input layer -> hidden layer (learns patterns)-> output layer (outputs learned,represented/probabilities)

    note: pattern<>embedding<>weights<>feature representation/vector
          input<> feature
    
    Learning:
        supervised: data + label
        semi-supervised: Data + some labels
        unsupervised : Data only
        transfer learning: using trained mdoel on other problem
    
    in this course:
        supervised learning and transfer learning
    
    some usecases of DL:
        Recomandation systems
        Tranlation
        speech recognition
        computer vision
        NLP
        sequence to sequence (seq2seq) like hindi to english
        classification and regression
    
    DL model <> DL architecture

    tensorflow:
        used to build DL mdoels.
        can write code in python and run on GPU/TPU
        have access to prebuild DL models (tensorflow hub)
        open-source
        it allows preprocessing,model,deploy model in your application.
        tensorflow.org
    
    TPU is for NN usually for tensorflow
    ASIC : application specific integrated circuit

    tensor:
        tensorflow<> flow of tensors
        vector is a tensor of rank 1
        tensor<> multidimetional matrix.
        Area is not a vector <> A vector can represent Area -> magnitude and perpendicular direction.
    

    using google colab:
        tensorflow-fundamentals.ipynb
        write comments in a cell and ctrl+M to convert it into text cell/markdown cell
        shift+enter for new cell
        esc+cmd +b from markdown cell to have next code cell
        tensorflow is a framwork.
        
        importing tensorflow
            import tensorflow as tf
            print(tf.__version__) #2.3.0
        it is very much like numpy

        creating tensor with tf.constant
            scalar=tf.constant(7) //ctrl+shift+space for documentation
            input to tf.constant can be any tensor like object
            scalar.ndim //0

            creating a vector 
                vector=tf.constant([1,2,3])
                vector.ndim is 1
            ndim is the length of its shape

            create a matrix
            matrix=tf.constant([[1,2,3],[4,5,6]])
            shape is (2,3) and ndim is 2

            data type would be most expressive one.
            
            matrix=tf.constant([[],[]],dtype=tf.float16) //using dtype parameter
                can use int32,int64,float64 etc
            
            if shape is 2,3,4 then it has 4 columns and 3 rows and 2 2d matrices

            basically ndim gives number of dimentions it takes in space.

            a tensor is an n-dimentional array of numbers,where n can be from 0 to inf
            1D tensor is a vector
            if in doubt,code it out.

            CREATING TENSORS WITH tf.Variable
                 these ones are changeable
            tensor=tf.Varialbe(your object)
            tensor[0].assign(7) to change a Variable tensor. can't change one created using constant.

            CREATING RANDOM TENSORS:
                try always tf.constant initially and only make variable when changes are needed.
                we use them to initialize the weights.
                random_1=tf.random.Generator.from_seed(42)
                random=random_1.normal(shape=(3,2))
                ---------------.uniform(----------)
                or use 
                    tf.random.normal(shape=...)
                    tf.random.uniform(shape=...)
                
                from a seed,you always get same tensor,given the same distribution,hence gives us reproduceability.

            SHUFFLING THE ORDER OF TENSORS.
                tf.random.shuffle(t1)
                    it shuffles randomly across first dimention                
                tf.random.shuffle(t2,seed=42)
                     to get same shuffled always.
                     keep global seed also 42 e.g tf.random.set_seed(42)
                     have to set global seed as well
                     important if you want it to be reproducible

                we shuffle to get random distribution of categories.
                so it doesn't learn only one type of pattern.

            SEED:
                operations that rely on a random seed derive from two seeds.
                global level
                opeartion level

                if both of them are set both seeds are used in conjunction to determine the random sequence.that is why shuffle with seed was giving different result.

                tf.random.set_seed(42) to set the global seed

            tf also has numpy like functions.
                tf.ones([2,3]) can use tuple also
                tf.zeros(shape=(2,3))

            NUMPY TO TENSOR
                the main difference between numpy and tf is that tf runs on GPU
                tensor=tf.constant(arr)
                capital letters for naming constant tensors

                tensor=tf.constant([],shape=(2,3,4)) element numbers should be same for reshaping
            
            in colab Runtime,Run before to run all before.

            GETTING INFO ABOUT TENSORS
                t.ndim
                t.shape
                tf.size(tensor) total no. of items in tensor

                two way to get something or do something
                 either t.property or function
                 or tf.function(tensor name)
                
            INDEXING
                A[0,3:4,5:6]
                A[0] first element in first dimetion but whole
                A.dtype
                A.shape[0]
                A.numpy() to convert into numpy
                A[...,tf.new axis] adds one more axis (2,2) to (2,2,1)   ... is same as  :,:,:,: and so on.
                or tf.expand_dims(rank2,axis=-1) 2,2 to 2,2,1
                if axis=0 then to 1,2,2 just extra []


    TENSOR OPERATIONS:

        tensor+10  original is unchanged
        tensor=tensor+10

        * / + - % work fine

        or tf.multiply(tensor,10) <> tf.math.multiply()
        it uses gpu and,orignal remains unchanged.
        tf.math.add() you can skip math for some opeartions it is a shorthand.

        basically to run on gpu use inbuilt functions.


        matrix multiplication:
            dot product of rows of first and columns of second.
            * is for element wise multiplication
            tf.matmul(t1,t2)
            tf.linalg.matmul(t1,t2)
            usually you can drop middle one.

            @ in python fo matrix multiplication
            t1@t2 for list.

            tf.reshape(y,shape=(2,3))
            tf.transpose(t)
            tf.matmul(tf.transpose(x),x)
        data types usually float32,int32
        16 bit precision is 3 times faster and less memory
        B=tf.cast(t,dtype=tf.float16) it is called reduced precision


        aggregates:
            condensing from more to less values
                abs:
                 to get absolute values:
                    tf.abs(D)
                tf.reduce_min(t)
                tf.reduce_max(t)
                tf.reduce_mean(t)
                tf.reduce_sum(t)
                tf.math.reduce_std(t) here t should be real 
                tf.math.reduce_variance(t)

        positional aggregates:
             tf.argmax(t) return index of max 
             np.argmax(A) also works
             similarly argmin() 

        squeezing tensor

            removing single dimentions
                tf.sqeeze(t) removes all 1 dimentions

        one hot encoding:

            l=[0,1,2,3]
            tf.one_hot(l,depth=4) we get 4,4 matrix with first columns first element 1 rest 0 and second columns 2nd and so on
            tf.one_hot(l,depth=4,on_value="hithere",off_value="hello")
            l is the indices in depth

            
        h=tf.range(1,10)
        tf.square(h)
        tf.sqrt(t2)
        tf.log(h)

        tensor numpy compatibility
            tf.constant(np.array(...))
            tensor.numpy(),np.array(tensor)
        

    using GPU/TPU
        tf.config.list_physical_devices() to see where we are running our program. 
        tf.config.list_physical_devices("GPU")
        list of GPU's,would return empty list if none

        go to runtime,change runtime type, click on GPU 
        now it will have GPU also

        CUDA -> DRIVER-> API by nvidia .inferface between tf and GPU
        !nvidia-smi to see gpu
        if you have access to CUDA enabled GPU, tf will automatically use it whenever needed.


module 3: neural network regression with tensorflow
    

    regression means output belongs to R 
    how much,how many kind of questions,or co-ordinates of image.
    
    for help go to tensorflow documentation.

    always use one hot encoding for categorical data,even if order exists.

    inputs<>input features.
    inputs are the input layer,hence shape is same as the no. of inputs.
    usually activation is Relu,actifvation could be put as None

    very often algorithms already exist.yo only do new things in research.

    import tensorflow as tf
    X is usual name for input feature matrix
    y usual output to be predicted.
    plt.scatter(X,y)

    input and output shapes:
        no. of columns
            like if 3 features (3,)
                                3 columns ,it cannot be no. of rows as then we would need atleast 1 column hence it will be 2D tensor like [[1],[2]]
        
        if 1 feature to predict 1 output
            (1,)                  (1,)
        
    steps in modelling with tensorflow:
        create a model:
            define the input and output layers,as well as the hidden layers of DL model.
        compiling a model:
            define the loss function,optimizer and evaluation matrix.
        fitting a model

    input and output should be tensors.

    model=tf.keras.Sequential([
        tf.keras.layers.Dense(1)  //here 1 is the no. of neurons.
    ]) 
    model.compile(loss=tf.keras.losses.mae,
                  optimizer=tf.keras.optimizers.SGD(), or optimizer="sgd"
                  metrics=["mae"]
            ) 
            mae: mean absolute error
            sgd: stochastic gradient descent.
            metrics is an evaluation metric.
    model.fit(X,y,epochs=5)
    his github mrdbourke
    
    plt.scatter(x,y)

    always set random seeds before for reproduceability.

    y_true and y_pred are cool names for those variables.

    Sequential groups  a linear stack of layers into a tf.keras.Model.
    optionally first layer can recieve an input_size parameter
        model=tf.keras.Sequential()
        model.add(tf.keras.layers.Dense(8,input_size(16,)))
        model.add(another layer)

    if layer uses float32 by default,cast it so, : if you get warning

    to predict from a model
        model.predict([17.0])
    
    we can improve model in:
        model part //make model smaller or larger,adding or removing layers or neurons.
        compiling part //optimizer from sgd to Adam,adam is amazing,and changing lr 
        fitting part //more epochs.
            
            tf.keras.layers.Dense(100,activation="relu")
            optimizer=tf.keras.optimizers.Adam(lr=0.001) //it is optimizers not optimizer in r-value
        when trying to improve a model try one thing at a time,otherwise you won't know what actually worked.

    try your model on small subset of data before running for whole data.
    
    loss="mae"
    metrics=["mae"]

    learning rate is the most important hyperparameter.


    evalutaion the peformance of a model:
        a typical workflow
            build a model
            fit it
            evaulate
            tweak it
            evaulate it
            tweat it
            evaulate it
            in loop
        what we tweak are called hyperparameters
            paremeters that we set
        when it comes to model,experiment,experiment,experiment.

        when it comes to evaluation:
            it is all about visualization.
                visualize the data,model,training,predictions,evaultions.
    
            X=tf.range(-100,100,4) to use range in tf.

        the three sets:
            train 
            test usually, 80 20 split

            can use:
                training set 70-80% of Data
                validation set  about 10-15 % //like a practise set before final exam.
                test set 10-15%
        splitting the data
            X_train = X[:40]
            y_train = y[:40]
            X_test = X[40:]
            y_test = y[40:]

        plt.figure=(figsize=(10,7))
        plt.scatter(x,y,c="b",label="first")
        plt.scatter(x1,y2,c="g",label="second")  //c is for color
        plt.legend();

        sometimes you manually need to define input shape and it can't automatically figure it out,in first layer.
            tf.keras.layers.Dense(1,input_shape=[1]) if only one feature,and use 1, if each row in []
        Dense<> fully connected.
        model.summary() to see summary of model.tells you how many paremeters in model.
        non-trainable paremeters: of model which is already trained and you use it.like in transfer leanring.
    
        trainable params: ones that we can update.

        mode.fit(x,y,epochs=100,verbose=0) //different levels of verbose,0 means no output. 1 means for each one and 2 means one per epoch.

        visualizing model
            from tensorflow.keras.utils import plot_model
            plot_model(model=model,show_shape=True)//can use to_file="abc.jpg" also to save.
                it prints graph of model which is cool

        tf.keras.layers.Dense(...   ,name="second layer") makes model.summary() good.
            can even put name for whole model as well.
        
        plot graph between y_pred and y_true to see how well it is doing.

        create a function if something comes in every model.like plotting.

        model.predict works also on list of inputs.

        metrics for regression:
            mean absolute error //mean of absolute error //best one
            mean square error.  //mean of squared error.
            tf.keras.losses.MSE()
            another one is huber //less sensitive to outliers
                tf.keras.losses.Huber()

        model.evaulate(X_test,y_test)  //for test test to check accuracy and .predict during deployment.

        always make sure that tensors have appropriate shape.

    Running experiments to improve our model:
        "experiment and visulize"
        to improve model:
            we can get more data.
            we can change our model.
            train longer.
        
        plot predictions vs true

        squeezing a 2D matrix makes it 1D like an array.

        epoches do not necessarily increase performance.
            when overfitting.
        Tensorboard is a component of tensorflow library to help track modelling experiments.other same tool is weights and biases.

    saving our models:
        so we can use them outside of collab.
        there are two main formats to save to:
            The savedmodel format
            the hdf5 format.
        
        using saved model format:
            model.save("best_model")//it creates a directory.
        saving in hdf5:
            model.save("our_model.h5") //it creates a single file.
        can load in c during production.
        loading a file:
            new_model=tf.keras.models.load_model("best_model")
            you can even train it again from now.
            same for hdf5 type.
            whole state is stored.
        you can mount your google drive and copy you files in colab there.

    you still use pandas to standardise the data.
    pd.read_csv("url") also works.
    better use one hot encolding inside of assigning 1,2,3 as they don't make sense.
        category becomes a new binary feature.
    
        pd.get_dummies(df) does automatically for columns that qualify.

    X=df.drop("output_column",axis=1)
    y=df["output_column"]


    train test split
        from sklearn.model_selection import train_test_split   
        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=42)

        we can now directly use pd dataframes in model.fit(df1,df2) which is cool.
        pandas and tf are compatible.

        model.evaulate(X_test,y_test) gives you mae or MSE
    
    Adam is amazing optimizer

    if your neuron can only give output from 0-1 make sure desired output is also in same range.(standardise/normalize)

    history=model.fit(...)
    history is the loss curve.
    pd.DataFrame(history.history).plot()
    plt.ylabel("loss")
    plt.xlabel("epochs")

    plotting in pandas
        pd["c1"].plot(kind="hist")
    you do scaling/preprocessing in pandas.

    just by normalization you can improve performance.

neural network classification:
    classification problems:
        when output is from a finite set.
        e.g binary classification:
                spam and not spam
        multiclass classification:
            which food is in the picture.
        
        multilabel classification:
            when you have multiple labels for an input,eg what is this article all about,it selects multiple labels in output.
    
    in images we have 3 color channels as RGB

    input and output shapes:
        shape: for image [None,224,224,3]
                    or for batched ones
                    [32,224,224,3] where 32 is the batch size   [batchsize,width,height,color_channels]
                    32 is a very common batch size.
        output shape could be [3] for three classes.

    in architecture:
        tf.keras.Input(shape=(224,224,3)) //first layer. //just define the shape no neurons added.
        tf.keras.layers.Dense(3,activation="softmax") //last layer  //can use sigmoid for binary classification.

        while compiling:
            loss=tf.keras.losses.CategoricalCrossentropy() //BinaryCrossentropy() for binary classification
            optimizer would be adam
            metrics=["accuracy"]

        model.fit(X_train,y_train,epochs=5)
        model.evaluate(xtest,ytest)
    
    google a recipe for neural networks have standard steps.
    binary classification can be done with only one output neuron as well.
    you use model.evaulate on test data,to see how good it is doing.
    
    andrei karpathy is in tesla deep learning he has guide also.

    visualization of data is key to good modelling.
    
    it is important to introduce nonlinearity to make model fit data.

    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    