Digital Design and Computer Architecture
----------------------------------------

Lecture 1: Introduction and Basics
    The first important step in designing is to make it work.
    in this course: 
        How computers work, from the ground up.
    nowadays people are using hardware to accelerate specific jobs, instead of letting a general processor do everything.

    Why do we do computing?
        To solve problems, to gain insights, to invent things, to discover things.
    How does computer work?
        By orchestrating (to arrange or manipulate) electrons using laws of physics.
        Engine === Processor.
    Why faster processors?
        Because they are not fast enough right now.
    Computers have to be predictable.
    Hardware acceleration: Hardware designed for specific purpose to do something efficiently.

    finiteness == finite 
    definiteness == precise 
    effective computability == turing machine can do it.
    devices == transistors

    As devices are getting smaller and smaller, things might become unreliable, so we have to take care of that.

    Computer Architecture:
        Science and art of designing computing platforms (Hw, Sw, interface, programming model)
        traditionally only Hw and interface are talked about.
        It is art, as you predict what users might ask 8 years from now.

    processors can be general purpose or special purpose.
    different applications have different constraints etc space, power, time etc 
    redundancy helps to detect faults.
    Intel has designed a RAM that is non-volatile.
    New thing -> memory chips also doing computation for efficiency.
    TPU: tensor processing unit, doing matrix multiplication.
    
Lecture 2: Course goals and Logistics
    You and your research by Hamming is a nice book.
    Don't forget the fundamentals of a field, like ever.
    your design should be principled. i.e you have to follow the principles.
    Architecture should be based upon principle, and not upon precedent, it is easy to repricate what has been done before.
    A Byzantine fault (also Byzantine generals problem, interactive consistency, source congruency, error avalanche, Byzantine agreement problem, and Byzantine failure[1]) is a condition of a computer system, particularly distributed computing systems, where components may fail and there is imperfect information on whether a component has failed. The term takes its name from an allegory, the "Byzantine generals problem",[2] developed to describe a situation in which, in order to avoid catastrophic failure of the system, the system's actors must agree on a concerted strategy, but some of these actors are unreliable.
    In a Byzantine fault, a component such as a server can inconsistently appear both failed and functioning to failure-detection systems, presenting different symptoms to different observers. It is difficult for the other components to declare it failed and shut it out of the network, because they need to first reach a consensus regarding which component has failed in the first place. Byzantine fault tolerance (BFT) is the resiliency of a fault-tolerant computer system to such conditions.
    
Lecture 3:
    DRAM: dynamic ram
    SRAM: static ram
    usually refresh per 64ms and it is an overhead energy wise as well as peformance wise. This happens for DRAM only.
    